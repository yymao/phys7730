{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 15: Classification II: Decision Trees and Ensemble Methods\n",
    "\n",
    "*For the class on Wednesday, March 27th*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Comparing Classification Methods\n",
    "\n",
    "This plot that compares different classification methods was shown in class\n",
    "when we discussed how the predicted \"probability\" is calculated by each method.\n",
    "\n",
    "**Document your discussion in class below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Modified from https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.datasets import make_circles, make_classification, make_moons\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "names = [\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Logistic\",\n",
    "    \"SVM\",\n",
    "    \"Decision Tree\",\n",
    "]\n",
    "\n",
    "\n",
    "classifiers = [\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42),\n",
    "    KNeighborsClassifier(3),\n",
    "    LogisticRegression(),\n",
    "    SVC(),\n",
    "    DecisionTreeClassifier(max_depth=8, random_state=42),\n",
    "]\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
    ")\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [\n",
    "    make_moons(noise=0.3, random_state=0),\n",
    "    make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "    linearly_separable,\n",
    "]\n",
    "\n",
    "figure = plt.figure(figsize=(24, 9.5))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=42\n",
    "    )\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
    "    # Plot the testing points\n",
    "    ax.scatter(\n",
    "        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\n",
    "    )\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "\n",
    "        clf = make_pipeline(StandardScaler(), clf)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        DecisionBoundaryDisplay.from_estimator(\n",
    "            clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5, vmin=0, vmax=1, response_method=(\"predict_proba\" if hasattr(clf, \"predict_proba\") else \"predict\"), plot_method=\"pcolormesh\",\n",
    "        )\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(\n",
    "            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    "        )\n",
    "        # Plot the testing points\n",
    "        ax.scatter(\n",
    "            X_test[:, 0],\n",
    "            X_test[:, 1],\n",
    "            c=y_test,\n",
    "            cmap=cm_bright,\n",
    "            edgecolors=\"k\",\n",
    "            alpha=0.6,\n",
    "        )\n",
    "\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(\n",
    "            x_max - 0.3,\n",
    "            y_min + 0.3,\n",
    "            (\"%.2f\" % score).lstrip(\"0\"),\n",
    "            size=15,\n",
    "            horizontalalignment=\"right\",\n",
    "        )\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A Question**: How does each of these methods generate the “probability”? Document your discussion in class below (just 1-2 sentences each). Note the name(s) of your group partner(s).\n",
    "\n",
    "--- \n",
    "*// Write your answer here*\n",
    "\n",
    "- Gaussian Naive Bayes: \n",
    "- Discriminant Analysis: \n",
    "- Gaussian Process: \n",
    "- K-Nearest-Neighbor: \n",
    "- Logistic regression: \n",
    "- Support Vector Machine: \n",
    "- Decision Tree: \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Decision Trees and Random Forests\n",
    "\n",
    "In this example, we still use the [Iris data set](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html).\n",
    "Like in Lab 14, because we are using the data set only as a demonstration, we will use all data as training data,\n",
    "so again be aware that this is *not* a good practice when you have a real problem to tackle! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and plot the iris data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "n_classes = len(iris.target_names)\n",
    "for i in range(n_classes):\n",
    "    plt.scatter(X[Y==i, 0], X[Y==i, 1], color=plt.cm.Paired(i/(n_classes-1)), marker=\"os^\"[i], label=f\"{i}: {iris.target_names[i]}\")\n",
    "plt.legend()\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1. Visualizing Trees\n",
    "\n",
    "Here is the code that generates the tree diagram that we looked at together in class. \n",
    "You can use the plot above to inspect how the decision tree works (*no written response needed*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=2, criterion=\"gini\")\n",
    "clf.fit(X[:,:2], Y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 16))\n",
    "plot_tree(clf, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2. Use cross validation to find tree depth\n",
    "\n",
    "Here we use the cross validation method (seen in [Lab 13](./13)) to find the ideal tree depth for this data set, \n",
    "for both the decision tree and random forest methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_cv = GridSearchCV(DecisionTreeClassifier(), {\"max_depth\": np.arange(1, 11)})\n",
    "tree_cv.fit(X, Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_cv = GridSearchCV(RandomForestClassifier(), {\"max_depth\": np.arange(1, 11)})\n",
    "forest_cv.fit(X, Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tree_cv.cv_results_[\"param_max_depth\"].data, tree_cv.cv_results_[\"mean_test_score\"], '.-', label=\"tree\")\n",
    "plt.plot(forest_cv.cv_results_[\"param_max_depth\"].data, forest_cv.cv_results_[\"mean_test_score\"], 'x-', label=\"forest\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"mean test score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B2 Question**: Based on the plot above, for this data set, what tree depth would you choose for the decision tree method and for the random forest method respectively? \n",
    "\n",
    "\n",
    "--- \n",
    "*// Write your answer here*\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Ensemble Methods\n",
    "\n",
    "This example shows how to use an ensemble method with scikit-learn! \n",
    "**Complete the task and answer the question below** (the main purpose is to get you familiar with scikit-learn API for ensemble methods).\n",
    "\n",
    "**Task**: Add two more ensemble classifiers to the code (add to the `classifiers` list; keep the existing two). The specifications of the two classifiers are:\n",
    "1. Adaptive boosting (`AdaBoostClassifier`) with the Support Vector Machine Classifier (`SVC`) as the base estimator.\n",
    "2. Stacking method (`StackingClassifier`) with the Logistic Regression (`LogisticRegression`), Quadratic Discriminant Analysis (`QuadraticDiscriminantAnalysis`), and Random Forest (`RandomForestClassifier`, use the depth you found in Part B) as the three base estimators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    BaggingClassifier(KNeighborsClassifier(3), random_state=42),\n",
    "    VotingClassifier([\n",
    "        (\"nb\", GaussianNB()),\n",
    "        (\"gp\", GaussianProcessClassifier(random_state=42)),\n",
    "        (\"knn\", KNeighborsClassifier(3)),\n",
    "        (\"tree\", DecisionTreeClassifier(max_depth=5)),\n",
    "    ]),\n",
    "    # TODO: add your code here\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(1, len(classifiers), figsize=(4*len(classifiers),4))\n",
    "\n",
    "for i, clf in enumerate(classifiers):\n",
    "    clf.fit(X[:,:2], Y)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        X[:,:2],\n",
    "        cmap=plt.cm.Paired,\n",
    "        ax=ax[i],\n",
    "        response_method=\"predict\",\n",
    "        plot_method=\"pcolormesh\",\n",
    "        shading=\"auto\",\n",
    "        xlabel=iris.feature_names[0],\n",
    "        ylabel=iris.feature_names[1],\n",
    "    )\n",
    "    ax[i].scatter(X[:, 0], X[:, 1], c=Y, edgecolors=\"k\", cmap=plt.cm.Paired)\n",
    "    ax[i].set_title(clf.__class__.__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C Question**: Which of these four methods (the two existing and the two you added) seems to work the best for this particular data set? Explain your reasoning in a few sentences. \n",
    "\n",
    "--- \n",
    "*// Write your answer here*\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "**How to submit this notebook on Canvas?**\n",
    "\n",
    "1. Make sure all your answers, code, and desired results are properly displayed in the notebook.\n",
    "2. Save the notebook (press `Ctrl`+`s` or `Cmd`+`s`). The grey dot on the filename tab (indicating \"unsaved\") should disappear. \n",
    "3. Run the following cell.\n",
    "4. Upload the resulting HTML file to Canvas under the corresponding assignment. \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "! jupyter nbconvert --to html ./15.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
